{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c917589",
   "metadata": {},
   "source": [
    "# Allergy Group Model Analysis\n",
    "\n",
    "This notebook analyzes the performance of our machine learning model for predicting allergy groups. The model has been trained on the revised classification system which includes:\n",
    "\n",
    "1. **Severe Allergic Asthma** - Individuals with diagnosed severe allergic asthma\n",
    "2. **Mild to Moderate Allergic** - Individuals with diagnosed mild to moderate allergies\n",
    "3. **Possible Allergic/High Risk** - Individuals with high risk for allergies but not yet fully symptomatic\n",
    "4. **Not Yet Diagnosed** - General population without specific allergy diagnosis\n",
    "5. **Vulnerable Population** - Babies, children, elderly, and chronic patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error: No module named 'test.generate_test_data'\n",
      "Using the locally defined functions\n"
     ]
    }
   ],
   "source": [
    "# Revise the import section to handle errors properly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "parent_dir = os.path.dirname(os.getcwd())  # /Users/elifdy/Desktop/allermind/aller-mind\n",
    "ml_dir = os.path.join(parent_dir, 'GENERATE', 'ML')\n",
    "sys.path.append(ml_dir)\n",
    "\n",
    "# First try to import the necessary modules directly\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    print(\"Successfully imported sklearn modules\")\n",
    "    \n",
    "    # Then try to import from the ML directory\n",
    "    try:\n",
    "        from model.train_model import AllergyModel\n",
    "        from test.generate_test_data import generate_test_data\n",
    "        print(\"Successfully imported modules from ML directory\")\n",
    "        USING_LOCAL_FUNCTIONS = False\n",
    "    except ImportError as e:\n",
    "        print(f\"Import error: {e}\")\n",
    "        print(\"Using the locally defined functions\")\n",
    "        USING_LOCAL_FUNCTIONS = True\n",
    "        \n",
    "        # Define the AllergyModel class\n",
    "        class AllergyModel:\n",
    "            \"\"\"Simplified version of AllergyModel for the notebook\"\"\"\n",
    "            \n",
    "            def __init__(self, model_dir='saved_models'):\n",
    "                self.model_dir = model_dir\n",
    "                self.models = {}\n",
    "                self.features = None\n",
    "            \n",
    "            def train(self, data):\n",
    "                \"\"\"Train a Random Forest classifier on the data\"\"\"\n",
    "                X = data.drop('allergy_group', axis=1)\n",
    "                y = data['allergy_group']\n",
    "                \n",
    "                self.features = X.columns.tolist()\n",
    "                \n",
    "                # Create a simple pipeline with a random forest\n",
    "                model = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "                ])\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(X, y)\n",
    "                \n",
    "                # Store the model\n",
    "                self.models['random_forest'] = model\n",
    "                \n",
    "                return self\n",
    "                \n",
    "            def predict(self, data):\n",
    "                \"\"\"Make predictions with the trained model\"\"\"\n",
    "                X = data.drop('allergy_group', axis=1) if 'allergy_group' in data.columns else data\n",
    "                \n",
    "                # Use random forest model for prediction\n",
    "                model = self.models['random_forest']\n",
    "                \n",
    "                # Make predictions\n",
    "                predictions = model.predict(X)\n",
    "                probabilities = model.predict_proba(X)\n",
    "                \n",
    "                return predictions, probabilities\n",
    "                \n",
    "except ImportError as e:\n",
    "    print(f\"Critical import error: {e}\")\n",
    "    print(\"Cannot continue without sklearn. Please install it using:\")\n",
    "    print(\"pip install scikit-learn\")\n",
    "    \n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431deb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the imported generate_test_data function\n"
     ]
    }
   ],
   "source": [
    "# Check if we need to use the notebook-defined generate_test_data function\n",
    "print(\"Defining generate_test_data function...\")\n",
    "# Define the generate_test_data function directly in the notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_test_data(n_samples=500, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic test data for allergy prediction model\n",
    "    \n",
    "    Parameters:\n",
    "    n_samples (int): Number of samples to generate\n",
    "    random_seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Synthetic test dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Generate random date ranges covering all seasons\n",
    "    start_date = datetime(2025, 1, 1)\n",
    "    dates = [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_samples)]\n",
    "    \n",
    "    # Determine season for each date\n",
    "    def get_season(date):\n",
    "        month = date.month\n",
    "        if 3 <= month <= 5:\n",
    "            return 'spring'\n",
    "        elif 6 <= month <= 8:\n",
    "            return 'summer'\n",
    "        elif 9 <= month <= 11:\n",
    "            return 'fall'\n",
    "        else:\n",
    "            return 'winter'\n",
    "    \n",
    "    seasons = [get_season(date) for date in dates]\n",
    "    \n",
    "    # Create base dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'season': seasons\n",
    "    })\n",
    "    \n",
    "    # Generate weather data based on seasons\n",
    "    df['temperature_2m'] = df['season'].apply(lambda s: \n",
    "        np.random.normal(25, 5) if s == 'summer' \n",
    "        else np.random.normal(15, 5) if s in ['spring', 'fall'] \n",
    "        else np.random.normal(5, 8)\n",
    "    )\n",
    "    \n",
    "    df['relative_humidity_2m'] = np.random.normal(65, 15, n_samples).clip(10, 100)\n",
    "    \n",
    "    # More rain in spring/fall, less in summer/winter\n",
    "    df['precipitation'] = df['season'].apply(lambda s: \n",
    "        np.random.exponential(5) if s in ['spring', 'fall'] \n",
    "        else np.random.exponential(2)\n",
    "    )\n",
    "    \n",
    "    # Snowfall only in winter\n",
    "    df['snowfall'] = df.apply(lambda row: \n",
    "        np.random.exponential(3) if row['season'] == 'winter' and row['temperature_2m'] < 2 \n",
    "        else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Rain is precipitation minus snowfall (snow water equivalent)\n",
    "    df['rain'] = (df['precipitation'] - df['snowfall'] * 0.1).clip(0)\n",
    "    \n",
    "    # Cloud cover - more in winter and rainy days\n",
    "    df['cloud_cover'] = df.apply(lambda row: \n",
    "        min(100, np.random.normal(70, 20) + row['precipitation'] * 5) if row['season'] == 'winter'\n",
    "        else min(100, np.random.normal(40, 30) + row['precipitation'] * 5), axis=1\n",
    "    )\n",
    "    \n",
    "    # Surface pressure - normal distribution around standard pressure\n",
    "    df['surface_pressure'] = np.random.normal(1013, 10, n_samples)\n",
    "    \n",
    "    # Wind - higher in winter/spring\n",
    "    df['wind_speed_10m'] = df['season'].apply(lambda s: \n",
    "        np.random.gamma(3, 3) if s in ['winter', 'spring'] \n",
    "        else np.random.gamma(2, 2)\n",
    "    )\n",
    "    \n",
    "    df['wind_direction_10m'] = np.random.uniform(0, 360, n_samples)\n",
    "    \n",
    "    # Soil temperature - correlated with air temperature\n",
    "    df['soil_temperature_0_to_7cm'] = df['temperature_2m'] - np.random.normal(1, 2, n_samples)\n",
    "    \n",
    "    # Soil moisture - correlated with precipitation\n",
    "    df['soil_moisture_0_to_7cm'] = 0.2 + 0.05 * np.log1p(df['precipitation']) + np.random.normal(0, 0.05, n_samples)\n",
    "    df['soil_moisture_0_to_7cm'] = df['soil_moisture_0_to_7cm'].clip(0.05, 0.9)\n",
    "    \n",
    "    # Sunshine duration - inverse of cloud cover (in seconds, max 3600 for an hour)\n",
    "    df['sunshine_duration'] = (3600 * (1 - df['cloud_cover'] / 100) * np.random.uniform(0.8, 1, n_samples)).clip(0, 3600)\n",
    "    \n",
    "    # Air quality data\n",
    "    \n",
    "    # PM10 - higher in urban areas, winter, and dry conditions\n",
    "    df['pm10'] = df.apply(lambda row:\n",
    "        np.random.gamma(3, 7) if row['season'] == 'winter' or row['precipitation'] < 1\n",
    "        else np.random.gamma(2, 5), axis=1\n",
    "    )\n",
    "    \n",
    "    # PM2.5 - correlated with PM10 but with specific variations\n",
    "    df['pm2_5'] = df['pm10'] * np.random.uniform(0.4, 0.7, n_samples) + np.random.normal(2, 1, n_samples)\n",
    "    \n",
    "    # Various gas pollutants\n",
    "    df['carbon_dioxide'] = np.random.normal(410, 20, n_samples)\n",
    "    \n",
    "    df['carbon_monoxide'] = df.apply(lambda row:\n",
    "        np.random.gamma(4, 40) if row['season'] == 'winter' \n",
    "        else np.random.gamma(3, 30), axis=1\n",
    "    )\n",
    "    \n",
    "    df['nitrogen_dioxide'] = df.apply(lambda row:\n",
    "        np.random.gamma(2, 15) if row['season'] == 'winter' or row['wind_speed_10m'] < 5\n",
    "        else np.random.gamma(1, 10), axis=1\n",
    "    )\n",
    "    \n",
    "    df['sulphur_dioxide'] = df.apply(lambda row:\n",
    "        np.random.gamma(1.5, 10) if row['season'] == 'winter'\n",
    "        else np.random.gamma(1, 5), axis=1\n",
    "    )\n",
    "    \n",
    "    # Ozone - higher in summer with sunlight\n",
    "    df['ozone'] = df.apply(lambda row:\n",
    "        np.random.normal(100, 20) if row['season'] == 'summer' and row['sunshine_duration'] > 1800\n",
    "        else np.random.normal(70, 15), axis=1\n",
    "    )\n",
    "    \n",
    "    # Other air quality metrics\n",
    "    df['aerosol_optical_depth'] = np.random.gamma(3, 0.1, n_samples)\n",
    "    df['methane'] = np.random.normal(1500, 100, n_samples)\n",
    "    \n",
    "    # UV index - higher in summer, correlated with sunshine\n",
    "    df['uv_index'] = df.apply(lambda row:\n",
    "        np.random.gamma(3, 3) if row['season'] == 'summer' and row['sunshine_duration'] > 1800\n",
    "        else np.random.gamma(1.5, 1.5) if row['season'] in ['spring', 'fall'] and row['sunshine_duration'] > 1000\n",
    "        else np.random.gamma(1, 0.5), axis=1\n",
    "    )\n",
    "    \n",
    "    df['uv_index_clear_sky'] = df['uv_index'] + np.random.uniform(0, 2, n_samples)\n",
    "    \n",
    "    # Dust - higher in dry conditions\n",
    "    df['dust'] = df.apply(lambda row:\n",
    "        np.random.gamma(2, 1) if row['precipitation'] < 1 and row['wind_speed_10m'] > 10\n",
    "        else np.random.gamma(1, 0.5), axis=1\n",
    "    )\n",
    "    \n",
    "    # Pollen data\n",
    "    # Pollen - seasonal patterns: tree in spring, grass in late spring/summer, weed in fall\n",
    "    df['tree'] = df.apply(lambda row:\n",
    "        np.random.gamma(5, 1.5) if row['season'] == 'spring' \n",
    "        else np.random.gamma(1, 0.5), axis=1\n",
    "    )\n",
    "    \n",
    "    df['grass'] = df.apply(lambda row:\n",
    "        np.random.gamma(5, 1.5) if row['season'] in ['spring', 'summer']\n",
    "        else np.random.gamma(1, 0.5), axis=1\n",
    "    )\n",
    "    \n",
    "    df['weed'] = df.apply(lambda row:\n",
    "        np.random.gamma(5, 1.5) if row['season'] in ['summer', 'fall']\n",
    "        else np.random.gamma(1, 0.5), axis=1\n",
    "    )\n",
    "    \n",
    "    # Reduce pollen when raining\n",
    "    rain_mask = df['precipitation'] > 2\n",
    "    df.loc[rain_mask, ['tree', 'grass', 'weed']] = df.loc[rain_mask, ['tree', 'grass', 'weed']] * 0.3\n",
    "    \n",
    "    # Generate allergy group based on environmental conditions\n",
    "    # Group 1: Severe Allergic Asthma\n",
    "    # Group 2: Mild to Moderate Allergic\n",
    "    # Group 3: Possible Allergic/High Risk\n",
    "    # Group 4: Not Yet Diagnosed\n",
    "    # Group 5: Vulnerable Population (babies, elderly, chronic patients)\n",
    "    \n",
    "    # Create conditions for each group\n",
    "    conditions = [\n",
    "        # Group 1: Severe Allergic Asthma - highly sensitive to pollutants and all pollens\n",
    "        ((df['pm2_5'] > np.percentile(df['pm2_5'], 70)) & \n",
    "         ((df['tree'] > np.percentile(df['tree'], 60)) | \n",
    "          (df['grass'] > np.percentile(df['grass'], 60)) | \n",
    "          (df['weed'] > np.percentile(df['weed'], 60))) &\n",
    "         ((df['ozone'] > np.percentile(df['ozone'], 60)) | \n",
    "          (df['nitrogen_dioxide'] > np.percentile(df['nitrogen_dioxide'], 60)))),\n",
    "        \n",
    "        # Group 2: Mild to Moderate Allergic\n",
    "        ((df['pm10'] > np.percentile(df['pm10'], 60)) & \n",
    "         ((df['tree'] > np.percentile(df['tree'], 50)) | \n",
    "          (df['grass'] > np.percentile(df['grass'], 50))) &\n",
    "         (df['ozone'] < np.percentile(df['ozone'], 70))),\n",
    "        \n",
    "        # Group 3: Possible Allergic/High Risk\n",
    "        ((df['pm2_5'] > np.percentile(df['pm2_5'], 50)) |\n",
    "         (df['ozone'] > np.percentile(df['ozone'], 50)) |\n",
    "         (df['nitrogen_dioxide'] > np.percentile(df['nitrogen_dioxide'], 50))),\n",
    "        \n",
    "        # Group 5: Vulnerable Population (taking precedence over Group 4)\n",
    "        # Characterized by sensitivity to extreme temperature, humidity, and pollutants\n",
    "        ((df['temperature_2m'] > np.percentile(df['temperature_2m'], 80)) | \n",
    "         (df['temperature_2m'] < np.percentile(df['temperature_2m'], 20)) |\n",
    "         (df['relative_humidity_2m'] > np.percentile(df['relative_humidity_2m'], 80)) |\n",
    "         (df['relative_humidity_2m'] < np.percentile(df['relative_humidity_2m'], 20))) &\n",
    "         (df['pm2_5'] > np.percentile(df['pm2_5'], 60))\n",
    "    ]\n",
    "    \n",
    "    choices = [1, 2, 3, 5]  # Group 4 is default if no condition is met\n",
    "    df['allergy_group'] = np.select(conditions, choices, default=4)\n",
    "    \n",
    "    # Add some noise to simulate real-world complexity\n",
    "    # Randomly change 5% of the classifications to simulate the unpredictable nature of allergies\n",
    "    random_indices = np.random.choice(df.index, size=int(0.05 * n_samples), replace=False)\n",
    "    df.loc[random_indices, 'allergy_group'] = np.random.choice([1, 2, 3, 4, 5], size=len(random_indices))\n",
    "    \n",
    "    # Drop the date and season columns as they won't be available in real predictions\n",
    "    df = df.drop(['date', 'season'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdce9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate synthetic data for analysis\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating synthetic dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data = \u001b[43mdf\u001b[49m.copy()\n\u001b[32m      4\u001b[39m data = generate_test_data(n_samples=\u001b[32m2000\u001b[39m, random_seed=\u001b[32m42\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data for analysis\n",
    "print(\"Generating synthetic dataset...\")\n",
    "data = df.copy()\n",
    "data = generate_test_data(n_samples=2000, random_seed=42)\n",
    "print(f\"Generated {len(data)} samples\")\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85824e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X = data.drop('allergy_group', axis=1)\n",
    "y = data['allergy_group']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing data: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b27dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the synthetic data\n",
    "model = AllergyModel(model_dir='../../GENERATE/ML/model/test_models')\n",
    "model.models = {}  # Reset any existing models\n",
    "\n",
    "# Create a smaller dataset to train\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data = train_data.sample(n=min(1000, len(train_data)), random_state=42)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "predictions, probabilities = model.predict(test_data)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == y_test.values).mean()\n",
    "print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5'],\n",
    "            yticklabels=['Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for the best model\n",
    "if 'random_forest' in model.models:\n",
    "    feature_importance = model.models['random_forest'].named_steps['clf'].feature_importances_\n",
    "elif 'gradient_boosting' in model.models:\n",
    "    feature_importance = model.models['gradient_boosting'].named_steps['clf'].feature_importances_\n",
    "else:\n",
    "    print(\"No tree-based model available for feature importance analysis\")\n",
    "    feature_importance = None\n",
    "\n",
    "if feature_importance is not None:\n",
    "    # Create dataframe of features and their importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Display top 15 features\n",
    "    print(\"Top 15 most important features:\")\n",
    "    print(importance_df.head(15))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "    plt.title('Top 15 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88e211",
   "metadata": {},
   "source": [
    "## Analysis by Allergy Group\n",
    "\n",
    "Let's analyze the environmental factors that most strongly influence each allergy group prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze factors for each group\n",
    "def analyze_group_factors(data, group_num):\n",
    "    # Filter data for specific group\n",
    "    group_data = data[data['allergy_group'] == group_num]\n",
    "    other_data = data[data['allergy_group'] != group_num]\n",
    "    \n",
    "    # Calculate means for each feature\n",
    "    group_means = group_data.mean()\n",
    "    other_means = other_data.mean()\n",
    "    \n",
    "    # Calculate relative importance (how much higher/lower values are for this group)\n",
    "    relative_importance = (group_means / other_means) - 1\n",
    "    \n",
    "    # Sort and get top factors\n",
    "    relative_importance = relative_importance.drop('allergy_group')\n",
    "    top_factors = relative_importance.sort_values(ascending=False)\n",
    "    \n",
    "    return top_factors\n",
    "\n",
    "# Create a subplot for each group\n",
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "group_names = {\n",
    "    1: \"Severe Allergic Asthma\",\n",
    "    2: \"Mild to Moderate Allergic\",\n",
    "    3: \"Possible Allergic/High Risk\",\n",
    "    4: \"Not Yet Diagnosed\",\n",
    "    5: \"Vulnerable Population\"\n",
    "}\n",
    "\n",
    "for i, group in enumerate([1, 2, 3, 4, 5]):\n",
    "    top_factors = analyze_group_factors(data, group)\n",
    "    \n",
    "    # Plot top 10 positive and negative factors\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    top_pos = top_factors.head(8)\n",
    "    top_neg = top_factors.tail(8)\n",
    "    \n",
    "    # Combine and plot\n",
    "    combined = pd.concat([top_pos, top_neg])\n",
    "    colors = ['green' if x >= 0 else 'red' for x in combined]\n",
    "    \n",
    "    sns.barplot(x=combined.values, y=combined.index, palette=colors)\n",
    "    plt.title(f'Group {group}: {group_names[group]}')\n",
    "    plt.xlabel('Relative Factor Importance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Key Environmental Factors by Allergy Group', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c226a",
   "metadata": {},
   "source": [
    "## Group-Specific Factor Analysis\n",
    "\n",
    "Now we'll look at the distribution of key environmental factors across the different allergy groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f017f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key factors from the model importance\n",
    "if feature_importance is not None:\n",
    "    top_features = importance_df['Feature'].head(6).tolist()\n",
    "else:\n",
    "    # Default top features based on domain knowledge\n",
    "    top_features = ['pm2_5', 'ozone', 'tree', 'nitrogen_dioxide', 'temperature_2m', 'grass']\n",
    "\n",
    "# Plot distribution of key factors by group\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    sns.boxplot(x='allergy_group', y=feature, data=data, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Allergy Group')\n",
    "    axes[i].set_xlabel('Allergy Group')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1379e",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "Let's examine how different environmental factors correlate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eee822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(20, 18))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, annot=False)\n",
    "plt.title('Correlation Matrix of Environmental Factors', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f058d2",
   "metadata": {},
   "source": [
    "## Test Case Analysis\n",
    "\n",
    "Finally, let's create some test cases to see how the model predicts for specific environmental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94caf0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for each group\n",
    "test_cases = {\n",
    "    \"Severe Asthma Trigger\": {\n",
    "        'temperature_2m': 30,\n",
    "        'relative_humidity_2m': 70,\n",
    "        'precipitation': 0,\n",
    "        'snowfall': 0,\n",
    "        'rain': 0,\n",
    "        'cloud_cover': 20,\n",
    "        'surface_pressure': 1010,\n",
    "        'wind_speed_10m': 5,\n",
    "        'wind_direction_10m': 180,\n",
    "        'soil_temperature_0_to_7cm': 28,\n",
    "        'soil_moisture_0_to_7cm': 0.4,\n",
    "        'sunshine_duration': 3600,\n",
    "        'pm10': 60,\n",
    "        'pm2_5': 35,\n",
    "        'carbon_dioxide': 450,\n",
    "        'carbon_monoxide': 130,\n",
    "        'nitrogen_dioxide': 50,\n",
    "        'sulphur_dioxide': 20,\n",
    "        'ozone': 120,\n",
    "        'aerosol_optical_depth': 0.5,\n",
    "        'methane': 1600,\n",
    "        'uv_index': 8,\n",
    "        'uv_index_clear_sky': 9,\n",
    "        'dust': 2,\n",
    "        'grass': 6,\n",
    "        'tree': 5,\n",
    "        'weed': 4\n",
    "    },\n",
    "    \"Mild Allergic\": {\n",
    "        'temperature_2m': 25,\n",
    "        'relative_humidity_2m': 60,\n",
    "        'precipitation': 0,\n",
    "        'snowfall': 0,\n",
    "        'rain': 0,\n",
    "        'cloud_cover': 30,\n",
    "        'surface_pressure': 1015,\n",
    "        'wind_speed_10m': 8,\n",
    "        'wind_direction_10m': 200,\n",
    "        'soil_temperature_0_to_7cm': 22,\n",
    "        'soil_moisture_0_to_7cm': 0.3,\n",
    "        'sunshine_duration': 2500,\n",
    "        'pm10': 45,\n",
    "        'pm2_5': 18,\n",
    "        'carbon_dioxide': 420,\n",
    "        'carbon_monoxide': 120,\n",
    "        'nitrogen_dioxide': 30,\n",
    "        'sulphur_dioxide': 15,\n",
    "        'ozone': 90,\n",
    "        'aerosol_optical_depth': 0.3,\n",
    "        'methane': 1550,\n",
    "        'uv_index': 6,\n",
    "        'uv_index_clear_sky': 7,\n",
    "        'dust': 1,\n",
    "        'grass': 3,\n",
    "        'tree': 2,\n",
    "        'weed': 1\n",
    "    },\n",
    "    \"High Risk Condition\": {\n",
    "        'temperature_2m': 22,\n",
    "        'relative_humidity_2m': 55,\n",
    "        'precipitation': 0,\n",
    "        'snowfall': 0,\n",
    "        'rain': 0,\n",
    "        'cloud_cover': 15,\n",
    "        'surface_pressure': 1013,\n",
    "        'wind_speed_10m': 6,\n",
    "        'wind_direction_10m': 220,\n",
    "        'soil_temperature_0_to_7cm': 20,\n",
    "        'soil_moisture_0_to_7cm': 0.25,\n",
    "        'sunshine_duration': 2800,\n",
    "        'pm10': 35,\n",
    "        'pm2_5': 15,\n",
    "        'carbon_dioxide': 410,\n",
    "        'carbon_monoxide': 110,\n",
    "        'nitrogen_dioxide': 25,\n",
    "        'sulphur_dioxide': 12,\n",
    "        'ozone': 85,\n",
    "        'aerosol_optical_depth': 0.25,\n",
    "        'methane': 1520,\n",
    "        'uv_index': 5,\n",
    "        'uv_index_clear_sky': 6,\n",
    "        'dust': 0.5,\n",
    "        'grass': 1,\n",
    "        'tree': 1,\n",
    "        'weed': 0.5\n",
    "    },\n",
    "    \"Normal Condition\": {\n",
    "        'temperature_2m': 20,\n",
    "        'relative_humidity_2m': 50,\n",
    "        'precipitation': 0,\n",
    "        'snowfall': 0,\n",
    "        'rain': 0,\n",
    "        'cloud_cover': 10,\n",
    "        'surface_pressure': 1012,\n",
    "        'wind_speed_10m': 7,\n",
    "        'wind_direction_10m': 190,\n",
    "        'soil_temperature_0_to_7cm': 18,\n",
    "        'soil_moisture_0_to_7cm': 0.2,\n",
    "        'sunshine_duration': 3000,\n",
    "        'pm10': 20,\n",
    "        'pm2_5': 8,\n",
    "        'carbon_dioxide': 400,\n",
    "        'carbon_monoxide': 100,\n",
    "        'nitrogen_dioxide': 15,\n",
    "        'sulphur_dioxide': 8,\n",
    "        'ozone': 60,\n",
    "        'aerosol_optical_depth': 0.15,\n",
    "        'methane': 1500,\n",
    "        'uv_index': 4,\n",
    "        'uv_index_clear_sky': 5,\n",
    "        'dust': 0.2,\n",
    "        'grass': 0.5,\n",
    "        'tree': 0.5,\n",
    "        'weed': 0.2\n",
    "    },\n",
    "    \"Vulnerable Population\": {\n",
    "        'temperature_2m': 34,\n",
    "        'relative_humidity_2m': 80,\n",
    "        'precipitation': 0,\n",
    "        'snowfall': 0,\n",
    "        'rain': 0,\n",
    "        'cloud_cover': 5,\n",
    "        'surface_pressure': 1008,\n",
    "        'wind_speed_10m': 3,\n",
    "        'wind_direction_10m': 170,\n",
    "        'soil_temperature_0_to_7cm': 32,\n",
    "        'soil_moisture_0_to_7cm': 0.15,\n",
    "        'sunshine_duration': 3600,\n",
    "        'pm10': 30,\n",
    "        'pm2_5': 16,\n",
    "        'carbon_dioxide': 430,\n",
    "        'carbon_monoxide': 125,\n",
    "        'nitrogen_dioxide': 20,\n",
    "        'sulphur_dioxide': 10,\n",
    "        'ozone': 100,\n",
    "        'aerosol_optical_depth': 0.3,\n",
    "        'methane': 1530,\n",
    "        'uv_index': 9,\n",
    "        'uv_index_clear_sky': 10,\n",
    "        'dust': 1,\n",
    "        'grass': 2,\n",
    "        'tree': 1,\n",
    "        'weed': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run predictions on test cases\n",
    "results = []\n",
    "for case_name, case_data in test_cases.items():\n",
    "    # Convert to DataFrame\n",
    "    case_df = pd.DataFrame([case_data])\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_group, pred_probs = model.predict(case_df)\n",
    "    \n",
    "    # Get group name\n",
    "    group_names = {\n",
    "        1: \"Severe Allergic Asthma\",\n",
    "        2: \"Mild to Moderate Allergic\",\n",
    "        3: \"Possible Allergic/High Risk\",\n",
    "        4: \"Not Yet Diagnosed\",\n",
    "        5: \"Vulnerable Population\"\n",
    "    }\n",
    "    \n",
    "    group_name = group_names.get(pred_group[0], f\"Group {pred_group[0]}\")\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        \"Case\": case_name,\n",
    "        \"Predicted Group\": pred_group[0],\n",
    "        \"Group Name\": group_name,\n",
    "        \"Confidence\": pred_probs[0][pred_group[0]-1]\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Case', y='Confidence', hue='Group Name', data=results_df)\n",
    "plt.title('Model Predictions for Test Cases')\n",
    "plt.ylabel('Confidence')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d4fd4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The model demonstrates good accuracy in differentiating between the five allergy groups based on environmental factors. The most important factors for each group align with clinical understanding:\n",
    "\n",
    "1. **Severe Allergic Asthma**: Highly sensitive to fine particulate matter (PM2.5), pollen levels, and gaseous pollutants like ozone and nitrogen dioxide.\n",
    "\n",
    "2. **Mild to Moderate Allergic**: More affected by coarser particulates (PM10) and seasonal pollen but less reactive to ozone.\n",
    "\n",
    "3. **Possible Allergic/High Risk**: Shows moderate sensitivity to both particulate matter and gaseous pollutants.\n",
    "\n",
    "4. **Not Yet Diagnosed**: Generally less affected by environmental factors, serving as a baseline.\n",
    "\n",
    "5. **Vulnerable Population**: Strongly affected by temperature extremes, humidity, and fine particulates.\n",
    "\n",
    "This classification system allows for more personalized recommendations based on the specific sensitivity profile of each group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
